% Best practice in any data science problem is to consider a range of appropriate models
% Detail the alternative models
% State the pros and cons of each, including the justification for your chosen approach.
% Critical evaluation of your methods is key

\section{Survey of potential alternatives}

\textbf{Clustering} the one-hot encoded features to group similar items wasn't viable.
Recommenders return ranked lists, while clustering would have returned an unordered list of items
belonging to the same cluster as the one considered for similarity.
Furthermore, the number of clusters was unknown.
Even using a density-based technique to auto-discover them would not have helped
because similarity ranking requires every pair of items to have a score,
and grouping does not map with this concept.
Clustering was a coarse-grained discretisation of similarity,
and I needed a more granular approach where every item could be compared with everything else.

The intuition was to map the concept of similarity with a geometric interpretation
and calculate the \textbf{pairwise distance} between the vectors representing the items,
given a metric and a vector space relevant to the similarity measure.
So, I discarded clustering as a candidate option.

One-hot encoding doesn't use spatial proximity information to transform the categorical features into ones and zeros.
It's a transformation process that pivots the unique values of each original feature to be the new variables of the transformed vector.
If we project these raw vectors in a multidimensional space, we wouldn't be able to use their
relative position to each other as a similarity measure. Moreover, the high dimensionality of the vectors would have increased
the computational complexity.

To calculate the pairwise distance efficiently and produce a meaningful representation of similarity,
the vectors needed to be transformed in a denser and lower dimensional space,
a manifold embedded into the original high-dimensional ambient space.

I considered dimensionality reduction techniques such as \textbf{principal component analysis (PCA)},
\textbf{independent component analysis (ICA)} or \textbf{linear discriminant analysis (LDA)},
but there was a problem with them too.
Their job is to find a linear projection of the data,
but this is a strong assumption that misses important non-linear structures.
I didn't use PCA (which I was more familiar with),
but I didn't discard the idea of using dimensionality reduction.
I just needed a non-linear approach and turned my attention to \textbf{manifold learning}.

I could have chosen an entirely different encoding technique to generate lower-dimensional vectors
and avoid the curse of the dimensionality problem from the beginning, so I also investigated \textbf{feature hashing}.
A hash function is a non-invertible function that can map data of arbitrary size to fixed-size values typically used for constant lookups.
Hashing can also be used as a feature transformation technique for tabular data.
It can generate smaller vectors than one-hot encoding but doesn't encode any concept of similarity either.
I didn't adopt it because, in comparison, it has more hyperparameters, which increases its complexity.
In addition, although unlikely, it introduces the collision problem,
where two distinct inputs can be mapped to the same index in the same target domain.
This issue could be mitigated by choosing the latest and most robust algorithm to reduce the likelihood of that happening.
However, the trade-off was too computationally expensive for pre-processing.
The reality was that I needed a pre-processing technique to transform categorical features into a high-dimensional numerical vector
to artificially inflate the encoding dimension for the modelling technique I had in mind since the beginning to produce quality embeddings.
