% Detail what the minimum desirable outcome is and how success will be measured.

\section{Scope of the project and Key Performance Indicators}

The scope for this project was to build an end-to-end machine learning solution, able to produce non-personalised content-to-content
similarity recommendations, using Passport metadata tags as input.

The minimum viable outcome was to produce recommendations comparable with the ones currently in production.
A desired outcome was to increase user engagement. The integration with Passport would make the solution general,
and applicable to multimodal BBC content. If adopted, this solution would generate considerable savings.

I didn't have access to the actual cost for each AWS account used for C2C across the BBC, so I couldn't present a number to demonstrate the reduction.
But, if this solution is adopted by $N$ products for example, the final cost would be divided by a factor of $N$, which is a considerable improvement.

Comparability was a qualitative and subjective key performance indicator (KPI) that served as a compass, indicating whether the project was progressing towards the right direction.
It was evaluated by several technical and non-technical stakeholders, with a diverse domain knowledge and background. I built a rudimentary visualisation
tool that rendered the title, image, description and metadata of the seed programme and the top-K C2C recommendations.
The people involved, could give their subjective feedback on what was their perceived level of similarity of the output,
testing edge cases and common use cases with an expected outcome, sensitive recommendations (e.g. content recommendations for children accounts),
and discussing anomalies and/or surprising results.

To measure user engagement though, the solution needed to be A/B tested in production, with real data.
Unfortunately, there were too many moving parts outside my control that needed to happen, for me to even try and build a production-worthy version for A/B testing.
Adopting this as a KPI would have delayed the project, increasing the odds of failure.
To mitigate this risk, I had to decouple it from the success of the project.

I defined a hypothesis testable offline, within my area of control.
The hypothesis stated that long-term user engagement is not just about accuracy. It can be affected by increasing diversity in the recommended content.
A diverse set of recommendations generates new and unexpected results, with an increase in surprise and serendipity, pushing the user away from boredom.
This was an untested hypothesis, but grounded in active research on the topic, such as \cite{Kaminskas2016DiversitySN} and
\cite{duricic2023beyondaccuracyreviewdiversityserendipity}, that supported the initial statement, and made it less far-fetched.
For this reason, I defined a proxy metric that could measure diversity offline, pending a future A/B testing for the validation of the hypothesis,
which was pushed out of scope.
