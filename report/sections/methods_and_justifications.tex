% Provide high-level overview of the methods (e.g. regression) and tools used
% with justification explaining why these methods are suitable for the problem at hand.
% Justification may include factors such as the need for interpretability, transparency
% or the timescales you had to work with.

\section{Methods used \& justification}

Methods used:

One-Hot Encoding to transform the categorical metadata into a numerical vectors
Undercomplete autoencoder to compress the sparse array into a lower-dimensional embedding
Cosine similarity to calculate a similarity score for top-K content similarity

Tools:
Python as a programming language
Pandas for dataframe manipulation. Each row of the table corresponds to an item, and each column to an annotation. Multi-hot, because more than one element of the array can have 1.
Dask for out-of-core and out-of-memory operations. The iPlayer catalogue considered has roughly 1 year worht of data.
Scikit-learn for dataset splitting and cosine similarity calculation
TensorFlow and Keras for modelling the Neural Network
matplotlib and seaborn for visualisation
rdflib to fetch the RDF entity from the Ontology for visualisation purposes

Justification


The data in input is a list of JSON files containing the Passport tags for each item in the iPlayer catalogue. The annotations
are categorical, so they must be transformed
This set of tags is extrapolated into a dictionary data structure which is then transformed in a numerical table during a pre-processing phase where the rows represent the programmes and the columns the Passport tags. The numerical vectors are subsequently passed in input to an Autoencoder model, that is trained to reconstruct the input with minimal loss. The encoder part of the learned weights can now be used to generate the embeddings given a set of Passport tags. These embeddings are finally used to calculate the content-to-content similarity between each pair of items, using the cosine as a scoring measure. The result is a matrix of scores that can be used to recommend the top K similar programmes, by filtering the seed programme, sorting the related scores in descending order and returning the top K elements.

A passport tag consists of a predicate/value pair, where the predicate is a string that describes the meaning of the tag (e.g. "about", "narrativeTheme",  etc.) and the value is an RDF entity within the BBC Things Ontology. Each Passport tag has a zero-to-many relationships with the item, so I had to consider the dynamic nature of the input size. Also, the same entity value can appear in conjunction with different predicates for the same item. For example: "contributor:Jeremy Clarkson" and "about:Jeremy Clarkson" are two tags with the same value but with different meaning. So, I had to consider this case in the pre-processing phase, to keep the information intact.

For the pre-processing phase I used One-Hot Encoding. Each programme can be annotated with multiple tags, so it is more like a Multi-Hot encoding, where more than one element can be set to 1. This type of encoding may sound counterintuitive given that for high-cardinality data it generates a high-dimensional vector with all the "curse" that follows. But this is not an issue in this case, because the following step would take care of this.

For the embedding generation, I used an Encoder-Decoder ANN architecture, specifically, an Undercomplete Autoencoder model, built using TensorFlow and Keras. This model can learn latent features and compress the information in a smaller multidimensional vector of condensed numerical values for a more efficient processing. To do so, I extrapolate the Encoder portion of the trained Autoencoder. The input layer of this new model is the same as the Autoencoder, so it is fed with encoded Passport tags and the output layer correspond to the bottleneck layer of the Autoencoder which is how it is able to generate embeddings.
