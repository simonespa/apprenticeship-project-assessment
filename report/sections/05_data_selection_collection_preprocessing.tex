% This gives a good opportunity to briefly describe the data architecture of your company
% You must show you have considered all relevant data protection policies and regulations (both internal company policy and external laws or guidelines).
% If you use any data external to the company you must reference it.
% Justify the choice of the data and why it is relevant to the project
% State what tools you used to efficiently gather the data
% Outline briefly the data cleansing steps you have taken and the consideration of any source of error and bias

\section{Data selection, collection and pre-processing}

% Data architecture

News and Sports articles, iPlayer videos on-demand, Sounds podcasts etc. are annotated with the so-called Passport tags.
These tags describe any content produced by the BBC and can be used for retrieval (search) and filtering (recommendations).
They can be applied either manually by an editorial team with domain knowledge,
or semi-automatically by machine learning algorithms with human supervision.

Passport tags are distributed across the BBC via the universal content exposure and delivery (UCED) system,
a self-service metadata delivery platform that exposes data as a document stream for products to integrate with.
This platform provides different types of \textit{consumers} such as REST API, AWS S3 bucket, etc.
Passport documents are JSON objects that contain a property called ``\verb|taggings|'', an array of objects representing the
metadata annotations. These objects are described by wo properties: ``\verb|predicate|'' and ``\verb|value|''. They represent the name and the value of
a tag, and are expressed as URL-formatted strings, except for dates. The predicate is a class of the BBC Ontology \cite{BBC:Ontologies}
while the value can be a date or an entity defined as an RDF \cite{W3C:RDF,W3C:RDF:Concepts} document,
accessible in Turtle format \cite{W3C:Turtle} via the BBC Things API \cite{BBC:Things,BBC:Things:About,BBC:Things:API}.
These entities are linked to each other and/or to external resources, and are described by attributes and relationships, giving the
data a graph structure.

% Tools used to gather data

I decided not to integrate with UCED but to use batches of Passport files, manually collected and stored
on a local folder.
This was a trade-off that allowed me to develop the project with real data, while still keeping the costs down. Because the resources needed to
be created on two AWS accounts, I didn't want to pass the burden of maintenance to the team that owned them, without having tested
the feasibility of the solution first.

% Data protection policies and regulation

According to the UK GDPR \cite{UKGDPR}, content metadata doesn't represent personally identifiable information (PII). Nonetheless,
this data is regulated by internal BBC data governance policies and as such, it is encrypted at rest and in transit by default.
For this reason, no further actions were required during storage and processing.

% Data choice justification and relevance

I chose to use Passport because this data provides a set of tags shared across all content produced by the BBC,
making this a general solution that reduces duplications and ultimately costs. Passport provides a flexible and rich set of tags to describe
any content. Annotations can describe canonical information such as \textit{genre} and \textit{format}, but also things like the \textit{contributor}
featuring in the programme, the \textit{narrative theme}, the \textit{editorial tone}, what the content is \textit{about}
or what relevant entities are \textit{mentioned}, etc.

% Data preprocessing, errors, bias

During pre-processing, a list of JSON files was loaded into the pipeline and the tags extrapolated into a dictionary data structure.
The key of the dictionary was the programme ID (called \textit{PID}) and the value was another dictionary describing the annotations.
A programme can be tagged with the same predicate multiple times if it has different values,
while the same value (e.g. ``Music'') can be used by multiple predicates (e.g. \verb|about| or \verb|genre|).

The dictionary was transformed in a Pandas \textit{Dataframe}, where the rows represented the programmes and the columns the tags.
I used a MultiIndex \cite{Pandas:MultiIndex} for the columns because I needed to keep the duplicate values (2nd-level index) across the predicates
(1st-level index).
I then populated the cells of the \textit{Dataframe} with the value ``\verb|1|'' if the programme was annotated with the corresponding tag,
or ``\verb|0|'' otherwise.
This process generated one-hot encoded arrays. I initially started with a vectorisation approach known to be performant,
using the ``\verb|get_dummies|'' Pandas function. Surprisingly, it was slower and less scalable of the solution that
I adopted in the end.

A source of bias in the dataset was the ``\verb|mentions|'' tag. This is automatically generated by an algorithm that extracts
terms deemed important, appearing in the text of an article or the transcript of an audio/video content.
If something is ``mentioned'', doesn't necessarily describe what the content is about, because of the intrinsic ambiguities of natural languages.
Figure of speech devices such as metaphors, analogies, allegories, etc., alter the meaning of a sentence for stylistic effect and can
misrepresent what the content was really about. For example, if the phrase ``being over the moon'' is mentioned,
while referring to something unrelated to the topic of ``space'' and ``universe'',
and the term ``moon'' is extracted as a descriptor, it would be misleading.
To mitigate this source of bias, I dropped the tag in favour of ``\verb|about|'', another tag that describes what the content is really about.
This is manually added by a team of editorials who are trained to annotate content with relevant and topical entities.

A source of error was the encoding of metadata at inference time. If the annotations had unseen tags, they wouldn't be represented
by the embeddings, leading to poor performances. So, I decided to drop the new tags and encode only the ones the model was trained on, pending retraining to capture the new information.
Also, some programmes didn't have any annotations entirely. Adding them to the training data created a group of entries with all zeros.
If enough observations shared this uninformative characteristic, it could have been picked up by the model.
I decided to drop these programmes and include only the ones with at least one annotation.
